{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0087DeepLearningConv2DFashionMNIST.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be downloaded from:\n",
    "https://www.kaggle.com/datasets/dschettler8845/brats-2021-task1/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torchvision\n",
    "\n",
    "# Miscellaneous\n",
    "import math\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import HTML, Image\n",
    "from IPython.display import display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout, SelectionSlider\n",
    "from ipywidgets import interact\n",
    "\n",
    "import pytest\n",
    "import nibabel as nib\n",
    "import os\n",
    "from nibabel.testing import data_path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from skimage.util import montage \n",
    "from skimage.transform import rotate\n",
    "from sklearn import preprocessing as pre\n",
    "from keras.utils import to_categorical \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    " ```python\n",
    " vallToFill = ???\n",
    " ```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    " ```python\n",
    " # You need to start writing\n",
    " ????\n",
    " ```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "???\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "\n",
    "IMG_SIZE = (240,240,155)\n",
    "\n",
    "TENSOR_BOARD_BASE   = 'TB'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Auxiliary Modules for Google Colab\n",
    "if runInGoogleColab:\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataManipulation.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataVisualization.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DeepLearningPyTorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotLabelsHistogram, PlotMnistImages\n",
    "from DeepLearningPyTorch import TrainModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    " device = torch.device(dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain Tumor Image Classification using DL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numSamplesTrain = 0 #TBD after we know how many samples we have\n",
    "numSamplesTest  = 0 #TBD after we know how many samples we have\n",
    "trainTestPercentage = 0.2\n",
    "# Model\n",
    "dropP = 0.2 #<! Dropout Layer\n",
    "\n",
    "# Training\n",
    "batchSize   = 10\n",
    "numWork     = 2 #<! Number of workers\n",
    "nEpochs     = 30\n",
    "\n",
    "# Visualization\n",
    "numImg = 3\n",
    "\n",
    "# samples root directory\n",
    "root_dir = '/Users/ronenrayten/Downloads/archive2021/' #The directeroy of the original Kaggel archive. \n",
    "numpy_archive_path = '/Users/ronenrayten/Downloads/' #the root directory of the processed numpy samples \n",
    "image_path = numpy_archive_path +'new_archive_cat/images/'  # path to the processed numpy images \n",
    "label_path = numpy_archive_path +'new_archive_cat/labels/'  # path to the processed numpy labeles \n",
    "\n",
    "\n",
    "# image types\n",
    "img_types = ['t1','t1ce','t2','flair','seg']\n",
    "\n",
    "\n",
    "#number of pixels in an image\n",
    "p_image_size = IMG_SIZE[0]*IMG_SIZE[1]*IMG_SIZE[2]\n",
    "crop_size = (130,160,128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "\n",
    "* Read the Kaggle archive (https://www.kaggle.com/datasets/dschettler8845/brats-2021-task1) and process it to numpy images to be saved to the disk.\n",
    "* Those functions are used only once to generate the data.\n",
    "* If you have the processed data already skip this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions to be run once to generate the numpy files containing the images\n",
    "def get_file_lists():\n",
    "   files = os.listdir(root_dir)\n",
    "   \n",
    "   file_list = []\n",
    "   \n",
    "   for file_name in files:\n",
    "     if os.path.isdir(root_dir +file_name):\n",
    "      all_samples_dirs = os.listdir(root_dir + file_name +'/')\n",
    "      for nii_dir in all_samples_dirs:\n",
    "        if os.path.isdir(root_dir + file_name + '/'+nii_dir):\n",
    "           nii_files = os.listdir(root_dir +'/'+ file_name + '/'+nii_dir)\n",
    "           \n",
    "           local_dir = root_dir + file_name + '/'+nii_dir+'/'\n",
    "           sample_files=['','','','','']\n",
    "           for file in nii_files:\n",
    "            if '_t1.' in file: \n",
    "               sample_files[0] = local_dir+file\n",
    "            elif '_t1ce.' in file: \n",
    "               sample_files[1] = local_dir+file\n",
    "            elif '_t2.' in file: \n",
    "               sample_files[2] = local_dir+file\n",
    "            elif '_flair.' in file: \n",
    "               sample_files[3] = local_dir+file\n",
    "            elif '_seg.' in file: \n",
    "               sample_files[4] = local_dir+file\n",
    "            else:\n",
    "               print (file)\n",
    "           file_list.append(sample_files)\n",
    "\n",
    "   return np.array(file_list)\n",
    "\n",
    "def create_repository():\n",
    "    # Directory \n",
    "    archive_directory = \"new_archive_cat\"\n",
    "    \n",
    "    # Path \n",
    "    root_archive_path = os.path.join(numpy_archive_path, archive_directory) \n",
    "    os.mkdir(root_archive_path) \n",
    "    \n",
    "    # Directory \n",
    "    image_directory = \"images\"\n",
    "   \n",
    "    # Path \n",
    "    image_path = os.path.join(root_archive_path, image_directory) \n",
    "    os.mkdir(image_path) \n",
    "    \n",
    "    # Directory \n",
    "    label_directory = \"labels\"\n",
    "   \n",
    "    # Path \n",
    "    label_path = os.path.join(root_archive_path, label_directory) \n",
    "    os.mkdir(label_path) \n",
    "\n",
    "#test function\n",
    "def test_combined():\n",
    "#1. Normalize the images between 0 to 1\n",
    "      #t1\n",
    "      sample_files = ['/Users/ronenrayten/Downloads/archive2021/BraTS2021_Training_Data/BraTS2021_01666/BraTS2021_01666_t1.nii.gz',\n",
    "                      '/Users/ronenrayten/Downloads/archive2021/BraTS2021_Training_Data/BraTS2021_01666/BraTS2021_01666_t1ce.nii.gz',\n",
    "                      '/Users/ronenrayten/Downloads/archive2021/BraTS2021_Training_Data/BraTS2021_01666/BraTS2021_01666_t2.nii.gz',\n",
    "                      '/Users/ronenrayten/Downloads/archive2021/BraTS2021_Training_Data/BraTS2021_01666/BraTS2021_01666_flair.nii.gz',\n",
    "                      '/Users/ronenrayten/Downloads/archive2021/BraTS2021_Training_Data/BraTS2021_01666/BraTS2021_01666_seg.nii.gz']\n",
    "      t1 = nib.load(sample_files[0]).get_fdata()\n",
    "      t1_norm = t1.reshape(-1,t1.shape[-1])\n",
    "      t1_norm = pre.MinMaxScaler().fit_transform(t1_norm)\n",
    "      t1_norm = t1_norm.reshape(*IMG_SIZE)\n",
    "      \n",
    "      t1ce = nib.load(sample_files[1]).get_fdata()\n",
    "      t1ce_norm = t1ce.reshape(-1,t1ce.shape[-1])\n",
    "      t1ce_norm = pre.MinMaxScaler().fit_transform(t1ce_norm)\n",
    "      t1ce_norm = t1ce_norm.reshape(*IMG_SIZE)\n",
    "      \n",
    "      t2 = nib.load(sample_files[2]).get_fdata()\n",
    "      t2_norm = t2.reshape(-1,t2.shape[-1])\n",
    "      t2_norm = pre.MinMaxScaler().fit_transform(t2_norm)\n",
    "      t2_norm = t2_norm.reshape(*IMG_SIZE)\n",
    "      \n",
    "      flair = nib.load(sample_files[3]).get_fdata()\n",
    "      flair_norm = flair.reshape(-1,flair.shape[-1])\n",
    "      flair_norm = pre.MinMaxScaler().fit_transform(flair_norm)\n",
    "      flair_norm = flair_norm.reshape(*IMG_SIZE)\n",
    "      \n",
    "      #2. Get the mask file and change its labels to 0-3 \n",
    "      seg = nib.load(sample_files[4]).get_fdata()\n",
    "      seg = seg.astype(np.uint8)\n",
    "      seg[seg==4] = 3\n",
    "      \n",
    "      # 3. combine the images into one with 4 channels\n",
    "      combined_image = np.stack([t1_norm,t1ce_norm,t2_norm,flair_norm],axis=3)\n",
    "      \n",
    "      # 4. Crop the images to 125x125x125x4\n",
    "      combined_image= combined_image[IMG_SIZE[0]//2 - crop_size//2:IMG_SIZE[0]//2 +crop_size//2,\n",
    "                                     IMG_SIZE[1]//2 - crop_size//2:IMG_SIZE[1]//2 + crop_size//2,\n",
    "                                     IMG_SIZE[2]//2 - crop_size//2:IMG_SIZE[2]//2 + crop_size//2]\n",
    "      \n",
    "      #5. crop the mask\n",
    "      seg = seg[IMG_SIZE[0]//2 - crop_size//2:IMG_SIZE[0]//2 +crop_size//2,\n",
    "                IMG_SIZE[1]//2 - crop_size//2:IMG_SIZE[1]//2 + crop_size//2,\n",
    "                IMG_SIZE[2]//2 - crop_size//2:IMG_SIZE[2]//2 + crop_size//2]\n",
    "\n",
    "      return combined_image\n",
    "   \n",
    "\n",
    "#generate the iages as numpy arrays\n",
    "def generate_images(file_list):\n",
    "   \n",
    "   sample_number = 0\n",
    "   for sample_files in file_list:\n",
    "      #1. Normalize the images between 0 to 1\n",
    "      t1 = nib.load(sample_files[0]).get_fdata()\n",
    "      t1_norm = t1.reshape(-1,t1.shape[-1])\n",
    "      t1_norm = pre.MinMaxScaler().fit_transform(t1_norm)\n",
    "      t1_norm = t1_norm.reshape(*IMG_SIZE)\n",
    "      \n",
    "      t1ce = nib.load(sample_files[1]).get_fdata()\n",
    "      t1ce_norm = t1ce.reshape(-1,t1ce.shape[-1])\n",
    "      t1ce_norm = pre.MinMaxScaler().fit_transform(t1ce_norm)\n",
    "      t1ce_norm = t1ce_norm.reshape(*IMG_SIZE)\n",
    "      \n",
    "      t2 = nib.load(sample_files[2]).get_fdata()\n",
    "      t2_norm = t2.reshape(-1,t2.shape[-1])\n",
    "      t2_norm = pre.MinMaxScaler().fit_transform(t2_norm)\n",
    "      t2_norm = t2_norm.reshape(*IMG_SIZE)\n",
    "      \n",
    "      flair = nib.load(sample_files[3]).get_fdata()\n",
    "      flair_norm = flair.reshape(-1,flair.shape[-1])\n",
    "      flair_norm = pre.MinMaxScaler().fit_transform(flair_norm)\n",
    "      flair_norm = flair_norm.reshape(*IMG_SIZE)\n",
    "      \n",
    "      #2. Get the mask file and change its labels to 0-3 \n",
    "      seg = nib.load(sample_files[4]).get_fdata()\n",
    "      seg = seg.astype(np.uint8)\n",
    "      seg[seg==4] = 3\n",
    "      \n",
    "      # 3. combine the images into one with 4 channels\n",
    "      combined_image = np.stack([t1_norm,t1ce_norm,t2_norm,flair_norm],axis=3)\n",
    "      \n",
    "      # 4. Crop the images to 130x160x125x4\n",
    "      combined_image= combined_image[55:185,43:203,13:141]\n",
    "      \n",
    "      #5. crop the mask\n",
    "      seg = seg[55:185,43:203,13:141]\n",
    "      seg = to_categorical(seg,4)\n",
    "      \n",
    "      #6. save the files\n",
    "      np.save(image_path + 'sample_' + str(sample_number) + '.npy',combined_image)\n",
    "      np.save(label_path + 'label_' + str(sample_number) + '.npy',seg)\n",
    "      \n",
    "      sample_number +=1\n",
    "      \n",
    "def calc_mean_std():\n",
    "   file_list = os.listdir(image_path)\n",
    "   avg_list= np.zeros(len(file_list)*4)\n",
    "   avg_list = avg_list.reshape(len(file_list),4)\n",
    "   std_list= np.zeros(len(file_list)*4)\n",
    "   std_list = std_list.reshape(len(file_list),4)\n",
    "   i=0\n",
    "   for sample in file_list:\n",
    "    image = np.load(image_path+sample)\n",
    "    avg_list[i] = np.mean(image,axis=(0,1,2))\n",
    "    std_list[i] = np.std(image,axis=(0,1,2))\n",
    "    i+=1\n",
    "   \n",
    "   return np.mean((avg_list),axis=0),np.mean((std_list),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell if you want to create the numpy image repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n"
     ]
    }
   ],
   "source": [
    "#create_repository and generate images\n",
    "#create_repository()\n",
    "#files = get_file_lists()\n",
    "#generate_images(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mX = os.listdir(image_path)\n",
    "vY = os.listdir(label_path)\n",
    "mX.sort()\n",
    "vY.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_372.npy label_372.npy\n"
     ]
    }
   ],
   "source": [
    "print(mX[555],vY[555])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train samples: 1001\n",
      "Number of Test samples: 250\n"
     ]
    }
   ],
   "source": [
    "numSamplesTest = np.int32(np.round(len(mX) * trainTestPercentage))\n",
    "numSamplesTrain = len(mX) - numSamplesTest\n",
    "\n",
    "print(\"Number of Train samples:\",numSamplesTrain)\n",
    "print(\"Number of Test samples:\",numSamplesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset and Loader\n",
    "\n",
    "The dataset takes as parameters:\n",
    "* mX - path to the sample numpy image file\n",
    "* vY - path to the masked label file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###################################################################\n",
    "#\n",
    "# The format of the batch returned:\n",
    "# Image: batch_size x number of images (4) x image_size (240x240x155)\n",
    "# Label: batch_size x number of images (1) x image_size (240x240x155)\n",
    "###################################################################\n",
    "\n",
    "class ImageDatasetFromDisk(Dataset):\n",
    "    def __init__(self, mX, vY, transform=None, target_transform=None):\n",
    "        \n",
    "        self.images = mX\n",
    "        self.labels = vY\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = np.load(image_path+self.images[idx])\n",
    "        \n",
    "        label = np.load(label_path+self.labels[idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process Data\n",
    "\n",
    "This section normalizes the data to have zero mean and unit variance per **channel**.  \n",
    "It is required to calculate:\n",
    "\n",
    " * The average pixel value per channel.\n",
    " * The standard deviation per channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Standardization Parameters\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the mean per channel.\n",
    "# 2. Calculate the standard deviation per channel.\n",
    "\n",
    "#file_list = get_file_names_by_type(root_dir,'HGG')\n",
    "#µ  = [np.mean(mX[:,:p_image_size]),np.mean(mX[:,p_image_size:p_image_size*2]),np.mean(mX[:,p_image_size*2:p_image_size*3]),np.mean(mX[:,p_image_size*3:p_image_size*4])]\n",
    "#σ  = [np.std(mX[:,:p_image_size]),np.std(mX[:,p_image_size:p_image_size*2]),np.std(mX[:,p_image_size*2:p_image_size*3]),np.std(mX[:,p_image_size*3:p_image_size*4])]\n",
    "\n",
    "#===============================================================#\n",
    "\n",
    "\n",
    "µ = [0.32519394, 0.17367085, 0.18451846, 0.23585342]\n",
    "σ = [0.32613024, 0.17851571, 0.20511049, 0.25051804]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels - TBD\n",
    "\n",
    "#hA = PlotLabelsHistogram(vY, lClass = L_CLASSES_FASHION_MNIST)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training features data shape: 1001\n",
      "The training labels data shape: 1001\n",
      "The test features data shape: 250\n",
      "The test labels data shape: 250\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split - TBD\n",
    "\n",
    "#numClass = len(np.unique(vY))\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Split the data into train and test (Validation) data sets (NumPy arrays).\n",
    "# 2. Use stratified split.\n",
    "# !! The output should be: `mXTrain`, `mXTest`, `vYTrain`, `vYTest`.\n",
    "\n",
    "mXTrain, mXTest, vYTrain, vYTest = train_test_split(mX, vY, test_size = numSamplesTest, train_size = numSamplesTrain, shuffle = True)\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The training features data shape: {len(mXTrain)}')\n",
    "print(f'The training labels data shape: {len(vYTrain)}')\n",
    "print(f'The test features data shape: {len(mXTest)}')\n",
    "print(f'The test labels data shape: {len(vYTest)}')\n",
    "#print(f'The unique values of the labels: {np.unique(vY)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data set data len: 1001\n",
      "The test data set data len: 250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mXTrain.sort()\n",
    "vYTrain.sort()\n",
    "mXTest.sort()\n",
    "vYTest.sort()\n",
    "dsTrain = ImageDatasetFromDisk(mXTrain,vYTrain)\n",
    "dsTest = ImageDatasetFromDisk(mXTest,vYTest)\n",
    "\n",
    "print(f'The training data set data len: {(len(dsTrain))}')\n",
    "print(f'The test data set data len: {(len(dsTest))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transform - convert image to tensor\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert image in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "         \n",
    "        tImg = torch.from_numpy(image).permute(3,0,1,2)\n",
    "        \n",
    "        return tImg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Transformer\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Define a transformer which normalizes the data.\n",
    "oDataTrns = torchvision.transforms.Compose([  #<! Chaining transformations\n",
    "    ToTensor(),        #<! Convert to Tensor (4,130,160,125)\n",
    "    torchvision.transforms.Normalize(µ[-1], σ[-1]),   #<! Normalizes the Data (https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html)\n",
    "    ])\n",
    "\n",
    "oLblTrns = torchvision.transforms.Compose([  #<! Chaining transformations\n",
    "    ToTensor(),        #<! Convert to Tensor (1,130,160,125)\n",
    "    ])\n",
    "\n",
    "# Update the DS transformer\n",
    "dsTrain.transform = oDataTrns\n",
    "dsTrain.target_transform  = oLblTrns\n",
    "\n",
    "dsTest.transform = oDataTrns\n",
    "dsTest.target_transform  = oLblTrns\n",
    "#===============================================================#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "\n",
    "The dataloader is the functionality which loads the data into memory in batches.  \n",
    "Its challenge is to bring data fast enough so the Hard Disk is not the training bottleneck.  \n",
    "In order to achieve that, Multi Threading / Multi Process is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the train data loader.\n",
    "# 2. Create the test data loader.\n",
    "# !! Think about the values of `shuffle` and `batch_size` for the train / test.\n",
    "dlTrain  = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize,drop_last=True)\n",
    "dlTest   = torch.utils.data.DataLoader(dsTest, shuffle = False, batch_size = 2 * batchSize,drop_last=True)\n",
    "\n",
    "#===============================================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch features dimensions: torch.Size([10, 4, 130, 160, 128])\n",
      "The batch labels dimensions: torch.Size([10, 4, 130, 160, 128])\n"
     ]
    }
   ],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "tX, vY = next(iter(dlTrain)) #<! PyTorch Tensors\n",
    "\n",
    "print(f'The batch features dimensions: {tX.shape}')\n",
    "print(f'The batch labels dimensions: {vY.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1550b6b10>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (12, 6))\n",
    "ax.imshow(vY[0,0,:,:,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "This section build 3 different models to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Container\n",
    "lModels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "oModel = nn.Sequential(\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the 1st model.\n",
    "# 2. Use 3 layers.\n",
    "# !! You may use different kernel size, dropout probability, max pooling, etc...\n",
    "\n",
    "    nn.Identity(),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 1, out_channels = 30, kernel_size = 3, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 30, out_channels = 60, kernel_size = 3, bias = False),\n",
    "    nn.MaxPool2d(kernel_size = 2),\n",
    "    nn.BatchNorm2d(num_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "            \n",
    "    nn.Conv2d(in_channels = 60,  out_channels = 120, kernel_size = 3, bias = False),\n",
    "    nn.MaxPool2d(kernel_size = 2),\n",
    "    nn.BatchNorm2d(num_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(120, len(L_CLASSES_FASHION_MNIST)),\n",
    "#===============================================================#\n",
    ")\n",
    "\n",
    "print(torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')) #<! Added `kernel_size`\n",
    "\n",
    "# Append Model\n",
    "lModels.append(oModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "oModel = nn.Sequential(\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the 2nd model.\n",
    "# 2. Use 3 layers.\n",
    "# !! You may use different kernel size, dropout probability, max pooling, etc...\n",
    "\n",
    "    nn.Identity(),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 1, out_channels = 30, kernel_size = 5, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 30, out_channels = 60, kernel_size = 5, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "            \n",
    "    nn.Conv2d(in_channels = 60,  out_channels = 120, kernel_size = 5, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(120, len(L_CLASSES_FASHION_MNIST)),\n",
    "#===============================================================#\n",
    ")\n",
    "\n",
    "print(torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')) #<! Added `kernel_size`\n",
    "\n",
    "# Append Model\n",
    "lModels.append(oModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "oModel = nn.Sequential(\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the 3rd model.\n",
    "# 2. Use 3 layers.\n",
    "# !! You may use different kernel size, dropout probability, max pooling, etc...\n",
    "\n",
    "    nn.Identity(),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 1, out_channels = 30, kernel_size = 7, bias = False),\n",
    "    nn.MaxPool2d(kernel_size = 2),\n",
    "    nn.BatchNorm2d(num_features = 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 30, out_channels = 60, kernel_size = 5, bias = False),\n",
    "    nn.MaxPool2d(kernel_size = 2),\n",
    "    nn.BatchNorm2d(num_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "            \n",
    "    nn.Conv2d(in_channels = 60,  out_channels = 120, kernel_size = 3, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(120, len(L_CLASSES_FASHION_MNIST)),\n",
    "#===============================================================#\n",
    ")\n",
    "\n",
    "print(torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')) #<! Added `kernel_size`\n",
    "\n",
    "# Append Model\n",
    "lModels.append(oModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Guideline: The smaller the image gets, the deeper it is (More channels).   \n",
    "  The intuition, the beginning of the model learns low level features (Small number), deeper learns combinations of features (Larger number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU Availability\n",
    "\n",
    "runDevice   = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Loss & Score\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Define loss function\n",
    "# 2. Define score function.\n",
    "hL = nn.CrossEntropyLoss()\n",
    "hS = MulticlassAccuracy(num_classes = len(L_CLASSES_FASHION_MNIST))\n",
    "hL = hL.to(runDevice) #<! Not required!\n",
    "hS = hS.to(runDevice)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Build a loop to evaluate all models.\n",
    "# 2. Define a TensorBoard Writer per model to keep its score.\n",
    "# !! You may use `TrainModel()`.\n",
    "\n",
    "for ii, oModel in enumerate(lModels):\n",
    "    # Hyper Parameter Loop\n",
    "    oTBWriter = SummaryWriter(log_dir = os.path.join(TENSOR_BOARD_BASE, f'Model{(ii + 1):03d}'))\n",
    "    oModel = oModel.to(runDevice) #<! Transfer model to device\n",
    "    oOpt = torch.optim.AdamW(oModel.parameters(), lr = 6e-4, betas = (0.9, 0.99), weight_decay = 1e-3) #<! Define optimizer\n",
    "    oRunModel, lTrainLoss, lTrainScore, lValLoss, lValScore = TrainModel(oModel, dlTrain, dlTest, oOpt, nEpochs, hL, hS, oTBWriter)\n",
    "    oTBWriter.close()\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Run `tensorboard --logdir=TB` from the Jupyter notebook path.\n",
    "* <font color='green'>(**@**)</font> Optimize the model search to get above 92% accuracy in validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
