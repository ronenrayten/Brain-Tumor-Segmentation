{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/AIProgram/2024_02/0087DeepLearningConv2DFashionMNIST.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be downloaded from:\n",
    "https://www.kaggle.com/datasets/dschettler8845/brats-2021-task1/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torchvision\n",
    "\n",
    "# Miscellaneous\n",
    "import math\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from platform import python_version\n",
    "from platform import python_version\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import HTML, Image\n",
    "from IPython.display import display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout, SelectionSlider\n",
    "from ipywidgets import interact\n",
    "\n",
    "import pytest\n",
    "import nibabel as nib\n",
    "import os\n",
    "from nibabel.testing import data_path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from skimage.util import montage \n",
    "from skimage.transform import rotate\n",
    "from sklearn import preprocessing as pre\n",
    "from keras.utils import to_categorical \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought.\n",
    "\n",
    "Code Notations:\n",
    "\n",
    "```python\n",
    "someVar    = 2; #<! Notation for a variable\n",
    "vVector    = np.random.rand(4) #<! Notation for 1D array\n",
    "mMatrix    = np.random.rand(4, 3) #<! Notation for 2D array\n",
    "tTensor    = np.random.rand(4, 3, 2, 3) #<! Notation for nD array (Tensor)\n",
    "tuTuple    = (1, 2, 3) #<! Notation for a tuple\n",
    "lList      = [1, 2, 3] #<! Notation for a list\n",
    "dDict      = {1: 3, 2: 2, 3: 1} #<! Notation for a dictionary\n",
    "oObj       = MyClass() #<! Notation for an object\n",
    "dfData     = pd.DataFrame() #<! Notation for a data frame\n",
    "dsData     = pd.Series() #<! Notation for a series\n",
    "hObj       = plt.Axes() #<! Notation for an object / handler / function handler\n",
    "```\n",
    "\n",
    "### Code Exercise\n",
    "\n",
    " - Single line fill\n",
    "\n",
    " ```python\n",
    " vallToFill = ???\n",
    " ```\n",
    "\n",
    " - Multi Line to Fill (At least one)\n",
    "\n",
    " ```python\n",
    " # You need to start writing\n",
    " ????\n",
    " ```\n",
    "\n",
    " - Section to Fill\n",
    "\n",
    "```python\n",
    "#===========================Fill This===========================#\n",
    "# 1. Explanation about what to do.\n",
    "# !! Remarks to follow / take under consideration.\n",
    "mX = ???\n",
    "\n",
    "???\n",
    "#===============================================================#\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "\n",
    "IMG_SIZE = (240,240,155)\n",
    "\n",
    "TENSOR_BOARD_BASE   = 'TB'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Auxiliary Modules for Google Colab\n",
    "if runInGoogleColab:\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataManipulation.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DataVisualization.py\n",
    "    !wget https://raw.githubusercontent.com/FixelAlgorithmsTeam/FixelCourses/master/AIProgram/2024_02/DeepLearningPyTorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courses Packages\n",
    "\n",
    "from DataVisualization import PlotLabelsHistogram, PlotMnistImages\n",
    "from DeepLearningPyTorch import TrainModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    " device = torch.device(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/BRaTS_2021/BRaTS_2021_task_1/\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "def str_to_bool(s):\n",
    "    return s.lower() in ('true', '1')\n",
    "print(os.getenv(\"ROOT_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain Tumor Image Classification using DL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "numSamplesTrain = 0 #TBD after we know how many samples we have\n",
    "numSamplesTest  = 0 #TBD after we know how many samples we have\n",
    "trainTestPercentage = 0.2\n",
    "# Model\n",
    "dropP = 0.2 #<! Dropout Layer\n",
    "\n",
    "# Training\n",
    "batchSize   = 10\n",
    "numWork     = 2 #<! Number of workers\n",
    "nEpochs     = 30\n",
    "\n",
    "# Visualization\n",
    "numImg = 3\n",
    "\n",
    "# samples root directory\n",
    "root_dir = os.getenv(\"ROOT_DIR\") #The directeroy of the original Kaggel archive. \n",
    "numpy_archive_path = os.getenv(\"NUMPY_ARCHIVE_PATH\") #the root directory of the processed numpy samples \n",
    "image_path = numpy_archive_path +'new_archive_cat/images/'  # path to the processed numpy images \n",
    "label_path = numpy_archive_path +'new_archive_cat/labels/'  # path to the processed numpy labeles \n",
    "\n",
    "\n",
    "# image types\n",
    "img_types = ['t1','t1ce','t2','flair','seg']\n",
    "\n",
    "\n",
    "#number of pixels in an image\n",
    "p_image_size = IMG_SIZE[0]*IMG_SIZE[1]*IMG_SIZE[2]\n",
    "crop_size = (130,160,128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "\n",
    "* Read the Kaggle archive (https://www.kaggle.com/datasets/dschettler8845/brats-2021-task1) and process it to numpy images to be saved to the disk.\n",
    "* Those functions are used only once to generate the data.\n",
    "* If you have the processed data already skip this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions to be run once to generate the numpy files containing the images\n",
    "def get_file_lists():\n",
    "   files = os.listdir(root_dir)\n",
    "   \n",
    "   file_list = []\n",
    "   \n",
    "   for file_name in files:\n",
    "     if os.path.isdir(root_dir +file_name):\n",
    "      all_samples_dirs = os.listdir(root_dir + file_name +'/')\n",
    "      for nii_dir in all_samples_dirs:\n",
    "        if os.path.isdir(root_dir + file_name + '/'+nii_dir):\n",
    "           nii_files = os.listdir(root_dir +'/'+ file_name + '/'+nii_dir)\n",
    "           \n",
    "           local_dir = root_dir + file_name + '/'+nii_dir+'/'\n",
    "           sample_files=['','','','','']\n",
    "           for file in nii_files:\n",
    "            if '_t1.' in file: \n",
    "               sample_files[0] = local_dir+file\n",
    "            elif '_t1ce.' in file: \n",
    "               sample_files[1] = local_dir+file\n",
    "            elif '_t2.' in file: \n",
    "               sample_files[2] = local_dir+file\n",
    "            elif '_flair.' in file: \n",
    "               sample_files[3] = local_dir+file\n",
    "            elif '_seg.' in file: \n",
    "               sample_files[4] = local_dir+file\n",
    "            else:\n",
    "               print (file)\n",
    "           file_list.append(sample_files)\n",
    "\n",
    "   return np.array(file_list)\n",
    "\n",
    "def create_repository():\n",
    "    # Directory \n",
    "    archive_directory = \"new_archive_cat\"\n",
    "    \n",
    "    # Path \n",
    "    root_archive_path = os.path.join(numpy_archive_path, archive_directory) \n",
    "    os.mkdir(root_archive_path) \n",
    "    \n",
    "    # Directory \n",
    "    image_directory = \"images\"\n",
    "   \n",
    "    # Path \n",
    "    image_path = os.path.join(root_archive_path, image_directory) \n",
    "    os.mkdir(image_path) \n",
    "    \n",
    "    # Directory \n",
    "    label_directory = \"labels\"\n",
    "   \n",
    "    # Path \n",
    "    label_path = os.path.join(root_archive_path, label_directory) \n",
    "    os.mkdir(label_path) \n",
    "\n",
    "#test function\n",
    "def test_combined():\n",
    "#1. Normalize the images between 0 to 1\n",
    "      #t1\n",
    "      sample_files = [f'{root_dir}\\BraTS2021_Training_Data\\BraTS2021_01666\\BraTS2021_01666_t1.nii.gz',\n",
    "                      f'{root_dir}\\BraTS2021_Training_Data\\BraTS2021_01666\\BraTS2021_01666_t1ce.nii.gz',\n",
    "                      f'{root_dir}\\BraTS2021_Training_Data\\BraTS2021_01666\\BraTS2021_01666_t2.nii.gz',\n",
    "                      f'{root_dir}\\BraTS2021_Training_Data\\BraTS2021_01666\\BraTS2021_01666_flair.nii.gz',\n",
    "                      f'{root_dir}\\BraTS2021_Training_Data\\BraTS2021_01666\\BraTS2021_01666_seg.nii.gz']\n",
    "      t1 = nib.load(sample_files[0]).get_fdata()\n",
    "      t1_norm = t1.reshape(-1,t1.shape[-1])\n",
    "      t1_norm = pre.MinMaxScaler().fit_transform(t1_norm)\n",
    "      t1_norm = t1_norm.reshape(*IMG_SIZE)\n",
    "      \n",
    "      t1ce = nib.load(sample_files[1]).get_fdata()\n",
    "      t1ce_norm = t1ce.reshape(-1,t1ce.shape[-1])\n",
    "      t1ce_norm = pre.MinMaxScaler().fit_transform(t1ce_norm)\n",
    "      t1ce_norm = t1ce_norm.reshape(*IMG_SIZE)\n",
    "      \n",
    "      t2 = nib.load(sample_files[2]).get_fdata()\n",
    "      t2_norm = t2.reshape(-1,t2.shape[-1])\n",
    "      t2_norm = pre.MinMaxScaler().fit_transform(t2_norm)\n",
    "      t2_norm = t2_norm.reshape(*IMG_SIZE)\n",
    "      \n",
    "      flair = nib.load(sample_files[3]).get_fdata()\n",
    "      flair_norm = flair.reshape(-1,flair.shape[-1])\n",
    "      flair_norm = pre.MinMaxScaler().fit_transform(flair_norm)\n",
    "      flair_norm = flair_norm.reshape(*IMG_SIZE)\n",
    "      \n",
    "      #2. Get the mask file and change its labels to 0-3 \n",
    "      seg = nib.load(sample_files[4]).get_fdata()\n",
    "      seg = seg.astype(np.uint8)\n",
    "      seg[seg==4] = 3\n",
    "      \n",
    "      # 3. combine the images into one with 4 channels\n",
    "      combined_image = np.stack([t1_norm,t1ce_norm,t2_norm,flair_norm],axis=3)\n",
    "      \n",
    "      # 4. Crop the images to 125x125x125x4\n",
    "      combined_image= combined_image[IMG_SIZE[0]//2 - crop_size//2:IMG_SIZE[0]//2 +crop_size//2,\n",
    "                                     IMG_SIZE[1]//2 - crop_size//2:IMG_SIZE[1]//2 + crop_size//2,\n",
    "                                     IMG_SIZE[2]//2 - crop_size//2:IMG_SIZE[2]//2 + crop_size//2]\n",
    "      \n",
    "      #5. crop the mask\n",
    "      seg = seg[IMG_SIZE[0]//2 - crop_size//2:IMG_SIZE[0]//2 +crop_size//2,\n",
    "                IMG_SIZE[1]//2 - crop_size//2:IMG_SIZE[1]//2 + crop_size//2,\n",
    "                IMG_SIZE[2]//2 - crop_size//2:IMG_SIZE[2]//2 + crop_size//2]\n",
    "\n",
    "      return combined_image\n",
    "   \n",
    "def calc_mean_std():\n",
    "   file_list = os.listdir(image_path)\n",
    "   avg_list= np.zeros(len(file_list)*4)\n",
    "   avg_list = avg_list.reshape(len(file_list),4)\n",
    "   std_list= np.zeros(len(file_list)*4)\n",
    "   std_list = std_list.reshape(len(file_list),4)\n",
    "   i=0\n",
    "   for sample in file_list:\n",
    "    image = np.load(image_path+sample)\n",
    "    avg_list[i] = np.mean(image,axis=(0,1,2))\n",
    "    std_list[i] = np.std(image,axis=(0,1,2))\n",
    "    i+=1\n",
    "   \n",
    "   return np.mean((avg_list),axis=0),np.mean((std_list),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_T1: False | ENABLE_T1CE: True | ENABLE_T2: True | ENABLE_FLAIR: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the environment variables for enabling/disabling each channel\n",
    "ENABLE_T1 = str_to_bool(os.getenv('ENABLE_T1'))\n",
    "ENABLE_T1CE = str_to_bool(os.getenv('ENABLE_T1CE'))\n",
    "ENABLE_T2 = str_to_bool(os.getenv('ENABLE_T2'))\n",
    "ENABLE_FLAIR = str_to_bool(os.getenv('ENABLE_FLAIR'))\n",
    "print(f\"ENABLE_T1: {ENABLE_T1} | ENABLE_T1CE: {ENABLE_T1CE} | ENABLE_T2: {ENABLE_T2} | ENABLE_FLAIR: {ENABLE_FLAIR}\")\n",
    "\n",
    "IMG_SIZE = (240, 240, 155)  # Example size, replace with actual size\n",
    "image_path = './images/'  # Replace with actual path\n",
    "label_path = './labels/'  # Replace with actual path\n",
    "\n",
    "def generate_images(file_list):\n",
    "    sample_number = 0\n",
    "    start_time = time.time()\n",
    "    for sample_files in file_list:\n",
    "        combined_images_list = []\n",
    "\n",
    "        if ENABLE_T1:\n",
    "            t1 = nib.load(sample_files[0]).get_fdata()\n",
    "            t1_norm = t1.reshape(-1, t1.shape[-1])\n",
    "            t1_norm = pre.MinMaxScaler().fit_transform(t1_norm)\n",
    "            t1_norm = t1_norm.reshape(*IMG_SIZE)\n",
    "            combined_images_list.append(t1_norm)\n",
    "\n",
    "        if ENABLE_T1CE:\n",
    "            t1ce = nib.load(sample_files[1]).get_fdata()\n",
    "            t1ce_norm = t1ce.reshape(-1, t1ce.shape[-1])\n",
    "            t1ce_norm = pre.MinMaxScaler().fit_transform(t1ce_norm)\n",
    "            t1ce_norm = t1ce_norm.reshape(*IMG_SIZE)\n",
    "            combined_images_list.append(t1ce_norm)\n",
    "\n",
    "        if ENABLE_T2:\n",
    "            t2 = nib.load(sample_files[2]).get_fdata()\n",
    "            t2_norm = t2.reshape(-1, t2.shape[-1])\n",
    "            t2_norm = pre.MinMaxScaler().fit_transform(t2_norm)\n",
    "            t2_norm = t2_norm.reshape(*IMG_SIZE)\n",
    "            combined_images_list.append(t2_norm)\n",
    "\n",
    "        if ENABLE_FLAIR:\n",
    "            flair = nib.load(sample_files[3]).get_fdata()\n",
    "            flair_norm = flair.reshape(-1, flair.shape[-1])\n",
    "            flair_norm = pre.MinMaxScaler().fit_transform(flair_norm)\n",
    "            flair_norm = flair_norm.reshape(*IMG_SIZE)\n",
    "            combined_images_list.append(flair_norm)\n",
    "\n",
    "        # Get the mask file and change its labels to 0-3 \n",
    "        seg = nib.load(sample_files[4]).get_fdata()\n",
    "        seg = seg.astype(np.uint8)\n",
    "        seg[seg == 4] = 3\n",
    "\n",
    "        # Combine the images into one with multiple channels\n",
    "        combined_image = np.stack(combined_images_list, axis=3)\n",
    "\n",
    "        # Crop the images to 130x160x125xN\n",
    "        combined_image = combined_image[55:185, 43:203, 13:141]\n",
    "\n",
    "        # Crop the mask\n",
    "        seg = seg[55:185, 43:203, 13:141]\n",
    "        seg = to_categorical(seg, 4)\n",
    "\n",
    "        # Save the files\n",
    "        np.save(image_path + 'sample_' + str(sample_number) + '.npy', combined_image)\n",
    "        np.save(label_path + 'label_' + str(sample_number) + '.npy', seg)\n",
    "        print(f'Sample number {sample_number + 1} saved. Total samples {len(file_list)}')\n",
    "        sample_number += 1\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'Elapsed time: {elapsed_time/60} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell if you want to create the numpy image repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './images/sample_0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m create_repository()\n\u001b[0;32m      4\u001b[0m files \u001b[38;5;241m=\u001b[39m get_file_lists()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mgenerate_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 62\u001b[0m, in \u001b[0;36mgenerate_images\u001b[1;34m(file_list)\u001b[0m\n\u001b[0;32m     59\u001b[0m seg \u001b[38;5;241m=\u001b[39m to_categorical(seg, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Save the files\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msample_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample_number\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m np\u001b[38;5;241m.\u001b[39msave(label_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(sample_number) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, seg)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_number\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved. Total samples \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(file_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DimaG\\envs\\TechnionAiProg\\Lib\\site-packages\\numpy\\lib\\npyio.py:542\u001b[0m, in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    541\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 542\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './images/sample_0.npy'"
     ]
    }
   ],
   "source": [
    "#create_repository and generate images\n",
    "if str_to_bool(os.getenv('CREATE_REPOSITORY')):\n",
    "    create_repository()\n",
    "    files = get_file_lists()\n",
    "    generate_images(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if str_to_bool(os.getenv('TEST_COMBINED')):\n",
    "    test_combined()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mX = os.listdir(image_path)\n",
    "vY = os.listdir(label_path)\n",
    "mX.sort()\n",
    "vY.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mX[555],vY[555])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSamplesTest = np.int32(np.round(len(mX) * trainTestPercentage))\n",
    "numSamplesTrain = len(mX) - numSamplesTest\n",
    "\n",
    "print(\"Number of Train samples:\",numSamplesTrain)\n",
    "print(\"Number of Test samples:\",numSamplesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset and Loader\n",
    "\n",
    "The dataset takes as parameters:\n",
    "* mX - path to the sample numpy image file\n",
    "* vY - path to the masked label file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###################################################################\n",
    "#\n",
    "# The format of the batch returned:\n",
    "# Image: batch_size x number of images (4) x image_size (240x240x155)\n",
    "# Label: batch_size x number of images (1) x image_size (240x240x155)\n",
    "###################################################################\n",
    "\n",
    "class ImageDatasetFromDisk(Dataset):\n",
    "    def __init__(self, mX, vY, transform=None, target_transform=None):\n",
    "        \n",
    "        self.images = mX\n",
    "        self.labels = vY\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = np.load(image_path+self.images[idx])\n",
    "        \n",
    "        label = np.load(label_path+self.labels[idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process Data\n",
    "\n",
    "This section normalizes the data to have zero mean and unit variance per **channel**.  \n",
    "It is required to calculate:\n",
    "\n",
    " * The average pixel value per channel.\n",
    " * The standard deviation per channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Standardization Parameters\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the mean per channel.\n",
    "# 2. Calculate the standard deviation per channel.\n",
    "\n",
    "#file_list = get_file_names_by_type(root_dir,'HGG')\n",
    "#µ  = [np.mean(mX[:,:p_image_size]),np.mean(mX[:,p_image_size:p_image_size*2]),np.mean(mX[:,p_image_size*2:p_image_size*3]),np.mean(mX[:,p_image_size*3:p_image_size*4])]\n",
    "#σ  = [np.std(mX[:,:p_image_size]),np.std(mX[:,p_image_size:p_image_size*2]),np.std(mX[:,p_image_size*2:p_image_size*3]),np.std(mX[:,p_image_size*3:p_image_size*4])]\n",
    "\n",
    "#===============================================================#\n",
    "\n",
    "\n",
    "µ = [0.32519394, 0.17367085, 0.18451846, 0.23585342]\n",
    "σ = [0.32613024, 0.17851571, 0.20511049, 0.25051804]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels - TBD\n",
    "\n",
    "#hA = PlotLabelsHistogram(vY, lClass = L_CLASSES_FASHION_MNIST)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split - TBD\n",
    "\n",
    "#numClass = len(np.unique(vY))\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Split the data into train and test (Validation) data sets (NumPy arrays).\n",
    "# 2. Use stratified split.\n",
    "# !! The output should be: `mXTrain`, `mXTest`, `vYTrain`, `vYTest`.\n",
    "\n",
    "mXTrain, mXTest, vYTrain, vYTest = train_test_split(mX, vY, test_size = numSamplesTest, train_size = numSamplesTrain, shuffle = True)\n",
    "#===============================================================#\n",
    "\n",
    "print(f'The training features data shape: {len(mXTrain)}')\n",
    "print(f'The training labels data shape: {len(vYTrain)}')\n",
    "print(f'The test features data shape: {len(mXTest)}')\n",
    "print(f'The test labels data shape: {len(vYTest)}')\n",
    "#print(f'The unique values of the labels: {np.unique(vY)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mXTrain.sort()\n",
    "vYTrain.sort()\n",
    "mXTest.sort()\n",
    "vYTest.sort()\n",
    "dsTrain = ImageDatasetFromDisk(mXTrain,vYTrain)\n",
    "dsTest = ImageDatasetFromDisk(mXTest,vYTest)\n",
    "\n",
    "print(f'The training data set data len: {(len(dsTrain))}')\n",
    "print(f'The test data set data len: {(len(dsTest))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transform - convert image to tensor\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert image in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "         \n",
    "        tImg = torch.from_numpy(image).permute(3,0,1,2)\n",
    "        \n",
    "        return tImg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Transformer\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Define a transformer which normalizes the data.\n",
    "oDataTrns = torchvision.transforms.Compose([  #<! Chaining transformations\n",
    "    ToTensor(),        #<! Convert to Tensor (4,130,160,125)\n",
    "    torchvision.transforms.Normalize(µ[-1], σ[-1]),   #<! Normalizes the Data (https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html)\n",
    "    ])\n",
    "\n",
    "oLblTrns = torchvision.transforms.Compose([  #<! Chaining transformations\n",
    "    ToTensor(),        #<! Convert to Tensor (1,130,160,125)\n",
    "    ])\n",
    "\n",
    "# Update the DS transformer\n",
    "dsTrain.transform = oDataTrns\n",
    "dsTrain.target_transform  = oLblTrns\n",
    "\n",
    "dsTest.transform = oDataTrns\n",
    "dsTest.target_transform  = oLblTrns\n",
    "#===============================================================#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "\n",
    "The dataloader is the functionality which loads the data into memory in batches.  \n",
    "Its challenge is to bring data fast enough so the Hard Disk is not the training bottleneck.  \n",
    "In order to achieve that, Multi Threading / Multi Process is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the train data loader.\n",
    "# 2. Create the test data loader.\n",
    "# !! Think about the values of `shuffle` and `batch_size` for the train / test.\n",
    "dlTrain  = torch.utils.data.DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchSize,drop_last=True)\n",
    "dlTest   = torch.utils.data.DataLoader(dsTest, shuffle = False, batch_size = 2 * batchSize,drop_last=True)\n",
    "\n",
    "#===============================================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate on the Loader\n",
    "# The first batch.\n",
    "tX, vY = next(iter(dlTrain)) #<! PyTorch Tensors\n",
    "\n",
    "print(f'The batch features dimensions: {tX.shape}')\n",
    "print(f'The batch labels dimensions: {vY.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (12, 6))\n",
    "x = vY[0]\n",
    "x = np.argmax(x,axis=0)\n",
    "ax1.imshow(x[:,:,75])\n",
    "ax2.imshow(tX[0,3,:,:,75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "This section build 3 different models to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Container\n",
    "lModels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "oModel = nn.Sequential(\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the 1st model.\n",
    "# 2. Use 3 layers.\n",
    "# !! You may use different kernel size, dropout probability, max pooling, etc...\n",
    "\n",
    "    nn.Identity(),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 1, out_channels = 30, kernel_size = 3, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 30, out_channels = 60, kernel_size = 3, bias = False),\n",
    "    nn.MaxPool2d(kernel_size = 2),\n",
    "    nn.BatchNorm2d(num_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "            \n",
    "    nn.Conv2d(in_channels = 60,  out_channels = 120, kernel_size = 3, bias = False),\n",
    "    nn.MaxPool2d(kernel_size = 2),\n",
    "    nn.BatchNorm2d(num_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(120, len(L_CLASSES_FASHION_MNIST)),\n",
    "#===============================================================#\n",
    ")\n",
    "\n",
    "print(torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')) #<! Added `kernel_size`\n",
    "\n",
    "# Append Model\n",
    "lModels.append(oModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "oModel = nn.Sequential(\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the 2nd model.\n",
    "# 2. Use 3 layers.\n",
    "# !! You may use different kernel size, dropout probability, max pooling, etc...\n",
    "\n",
    "    nn.Identity(),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 1, out_channels = 30, kernel_size = 5, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 30, out_channels = 60, kernel_size = 5, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "            \n",
    "    nn.Conv2d(in_channels = 60,  out_channels = 120, kernel_size = 5, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(120, len(L_CLASSES_FASHION_MNIST)),\n",
    "#===============================================================#\n",
    ")\n",
    "\n",
    "print(torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')) #<! Added `kernel_size`\n",
    "\n",
    "# Append Model\n",
    "lModels.append(oModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Defining a sequential model.\n",
    "\n",
    "oModel = nn.Sequential(\n",
    "#===========================Fill This===========================#\n",
    "# 1. Create the 3rd model.\n",
    "# 2. Use 3 layers.\n",
    "# !! You may use different kernel size, dropout probability, max pooling, etc...\n",
    "\n",
    "    nn.Identity(),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 1, out_channels = 30, kernel_size = 7, bias = False),\n",
    "    nn.MaxPool2d(kernel_size = 2),\n",
    "    nn.BatchNorm2d(num_features = 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.Conv2d(in_channels = 30, out_channels = 60, kernel_size = 5, bias = False),\n",
    "    nn.MaxPool2d(kernel_size = 2),\n",
    "    nn.BatchNorm2d(num_features = 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "            \n",
    "    nn.Conv2d(in_channels = 60,  out_channels = 120, kernel_size = 3, bias = False),\n",
    "    nn.BatchNorm2d(num_features = 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p = dropP),\n",
    "    \n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(120, len(L_CLASSES_FASHION_MNIST)),\n",
    "#===============================================================#\n",
    ")\n",
    "\n",
    "print(torchinfo.summary(oModel, tX.shape, col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')) #<! Added `kernel_size`\n",
    "\n",
    "# Append Model\n",
    "lModels.append(oModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Guideline: The smaller the image gets, the deeper it is (More channels).   \n",
    "  The intuition, the beginning of the model learns low level features (Small number), deeper learns combinations of features (Larger number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU Availability\n",
    "\n",
    "runDevice   = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Loss & Score\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Define loss function\n",
    "# 2. Define score function.\n",
    "hL = nn.CrossEntropyLoss()\n",
    "hS = MulticlassAccuracy(num_classes = len(L_CLASSES_FASHION_MNIST))\n",
    "hL = hL.to(runDevice) #<! Not required!\n",
    "hS = hS.to(runDevice)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Build a loop to evaluate all models.\n",
    "# 2. Define a TensorBoard Writer per model to keep its score.\n",
    "# !! You may use `TrainModel()`.\n",
    "\n",
    "for ii, oModel in enumerate(lModels):\n",
    "    # Hyper Parameter Loop\n",
    "    oTBWriter = SummaryWriter(log_dir = os.path.join(TENSOR_BOARD_BASE, f'Model{(ii + 1):03d}'))\n",
    "    oModel = oModel.to(runDevice) #<! Transfer model to device\n",
    "    oOpt = torch.optim.AdamW(oModel.parameters(), lr = 6e-4, betas = (0.9, 0.99), weight_decay = 1e-3) #<! Define optimizer\n",
    "    oRunModel, lTrainLoss, lTrainScore, lValLoss, lValScore = TrainModel(oModel, dlTrain, dlTest, oOpt, nEpochs, hL, hS, oTBWriter)\n",
    "    oTBWriter.close()\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> Run `tensorboard --logdir=TB` from the Jupyter notebook path.\n",
    "* <font color='green'>(**@**)</font> Optimize the model search to get above 92% accuracy in validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
